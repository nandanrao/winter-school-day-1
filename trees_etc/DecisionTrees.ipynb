{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src = \"./bgsedsc_0.jpg\">\n",
    "\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Tree-based & additive models, Ensemble methods: forests, bagging and boosting\n",
    "\n",
    "This lecture introduces some fundamental concepts for effective predictive modelling based on the concept of decision trees. The combination of the models and algorithms developed here, combined also with linear models and regularisation, is what most often used in practice due to the resultant high prediction accuracy.  \n",
    "\n",
    "As you will see within this paradigm the features become part of the learning procedure - not merely by selection, as e.g. with lasso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Summary\n",
    "This module introduces both classification and regression models under the umbrella of **decision trees**. Those are non-linear models that work by iteratively splitting the space of one input feature (explanatory variable) in a way that the resulting two splits in training data have minimum variability in terms of the output value to predict. Combining the sequences of splits that the algorithm finds, one can represent those models as trees with branches and leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## We load the relevant modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to display the tree.\n",
    "# NOTE: requires pydotplus and graphviz libraries. \n",
    "#       for MACOS/LINUX just install by typing \"conda install pydotplus\"  (and graphviz too) at the terminal\n",
    "#       for Windows install by typing \"conda install pydotplus\" at the Anaconda Powershell Prompt \n",
    "#       for Windows and graphviz, if conda install doesn't work, then download the zip file from https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "#             unzip, include the 'bin' folder path in an environment variable named 'path' hen do 'pip install graphviz' and 'conda install graphviz' from anaconda prompt\n",
    "#             finally, also include in the environment variable 'path' the path to the Anaconda graphviz package, i.e. SomePath\\Anaconda3\\Library\\bin\\graphviz\n",
    "    \n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "import graphviz\n",
    "def plot_tree(clf, feature_names, target_names):\n",
    "    dot_data = sklearn.tree.export_graphviz(clf, out_file=None, \n",
    "                             feature_names=feature_names,  \n",
    "                             class_names= target_names,  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True) \n",
    "    return pydotplus.graph_from_dot_data(dot_data).create_png() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Linear models with tree-based features\n",
    "\n",
    "To work our way towards tree-based models consider the following linear model with **tree-based features** \n",
    "\n",
    "We have input $\\bx=(x_1,\\ldots,x_p)^T$ a vector of $p$ input variables. Then, \n",
    "\n",
    "$$f(\\bx;\\bb) = \\sum_{m=1}^p \\beta_m 1[\\bx \\in R_m]$$\n",
    "\n",
    "where the $R_m$ are disjoint subsets of the input space defined by a binary tree, say splitting each variable on the median. \n",
    "\n",
    "For example say $p=2$ and we split each variable to the median, then there would be four regions: \n",
    "+ region 1: $x_1 < median(x_1)$ and $x_2 < median(x_2)$\n",
    "+ region 2: $x_1 > median(x_1)$ and $x_2 < median(x_2)$\n",
    "+ region 2: $x_1 < median(x_1)$ and $x_2 > median(x_2)$\n",
    "+ region 2: $x_1 > median(x_1)$ and $x_2 > median(x_2)$\n",
    "\n",
    "Below, there is a more generic partitioned space of 2 variables\n",
    "\n",
    "<img src = \"tree_subregions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In terms of function approximation, the assumption here is that the regression function $f$ is constant in each of these regions, hence the data are split in homogenous subgroups. \n",
    "\n",
    "It is trivial to fit this regression model to data, each $\\beta_m$ is estimated from the training that fall in the subregion alone, and equals the sample mean of the data in that subregion. Hence, this is a simple instance of a linear model. \n",
    "\n",
    "However, there are (at least) two main problems with this approach:\n",
    "\n",
    "1. Computational: for $p$ variables there are $2^p$ regions if we split each variable in its median, hence we quickly end up with massive number of features - and will overfit unless we take precautions \n",
    "2. Predictive: even if we have carefully preselected the input variables in terms of their relevance for prediction it might turn out that a few, or even most, are not useful for prediction, hence we should not be subdiving the population according to these variables. On the other hand, for those that are relevant for prediction, there is no good reason why the median, or any preselected quantile, provides a good split. We would like to learn from the data itself where to split each variable (if used at all in the model); and maybe in order to get a good predictive model we might have to split some variables several times. Which now feeds back into the \"computational\" problem since the  potential regions might be much larger than $2^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tree based models: a first illustration\n",
    "\n",
    "Tree-based models, and the associated learning algorithms for fitting them to data, address these two challenges and result in a non-linear (in the parameters as well as the inputs) model where the variables to be split and the split points are learned from data. \n",
    "\n",
    "Before we discuss tree-based models and algorithms any further let's immediatelly implement one such algorithm to the spam training data. The objective of this particular example is to classify emails as 'spam' or not-spam ('email'), based on features that are basically the occurrence of certain words or expressions.\n",
    "\n",
    "What we are doing below is **bad practice**: never run a function you do not know what it does - especially when it has many options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load the spam\n",
    "dataset = pd.read_csv(\"spam_small_train.csv\")\n",
    "X = dataset.drop('class', axis = 1)\n",
    "y = dataset[\"class\"]\n",
    "\n",
    "# to print stats\n",
    "feature_names = X.columns\n",
    "class_labels = [\"email\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# plot the tree\n",
    "Image(plot_tree(model, feature_names, class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note the complexity of this tree. Forks to the right imply 'false' in the splitting criteria, forks to the left are 'true'. Orange colours mean output 'no spam', whereas blue colours mean 'spam'. The latter are predominant in the right hand side of the tree. You can zoom into the figure by saving (right-clicking) and opening elsewhere.\n",
    "\n",
    "Before we go any further, lets check the in and out of sample performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# read the test data, extract the info and create predictions\n",
    "\n",
    "spam_test = pd.read_csv(\"spam_small_test.csv\")\n",
    "Xtest = spam_test.drop(\"class\",axis=1)\n",
    "ytest = spam_test[\"class\"]\n",
    "pred = model.predict_proba(X)\n",
    "test_pred = model.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pred[:,1],\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(test_pred[:,1],\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We observe that the model predicts 'spam' or 'email' with apparently high confidence on its results (probabilities close to 0 or 1, few points in between)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### AUC recap\n",
    "Recall that, for binary classification models, to check performance we use the Area Under the Curve (**AUC**), considering the **ROC** curve (Receiver Operating Charateristic). Remember that ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes if we use different thresholds of probability. In general, the higher the AUC (from 0 to 1), the better the model is at predicting a binary classification (in this case 'spam' and 'email'. \n",
    "\n",
    "The curve plots the relationship between True Positive Rate and False Positive Rate, having these definitions:\n",
    "\n",
    "$TPR=Sensitivity=Recall=\\frac{TP}{TP+FN}$\n",
    "\n",
    "$FPR=1-Specificity=\\frac{FP}{FP+TN}$\n",
    "\n",
    "If AUC is 1 (the maximum), it means we can isolate and fully predict one class (e.g. 'spam') without any missclassficaction (e.g. 'emails' classified as 'spam'). Normally, the cost of having high sensitivity (get all 'spam' classified correctly) is having also a relatively high FPR (classify incorrectly as 'spam' some 'email')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Custom plot function\n",
    "get_auc(y, pred, class_labels, column=1, plot=True) # Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Custom plot function\n",
    "get_auc(ytest, test_pred, class_labels, column=1, plot=True) # Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are clear signs of overfitting here, because AUC is 1 for training set, but clearly smaller for test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Understanding the model and the algorithm\n",
    "\n",
    "From the Hastie et al. book, here is an example of the binary tree representation of the tree-based model and the corresponding partition of input space into subregions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src = \"binary_tree.png\"> <img src = \"tree_subregions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this method to predictive modelling the model and the algorithms are pretty much interweaved. The loss function can be used pretty much as in our previous supervised learning approaches. However, here it is not entirely straightforward (but can be done!) to write down a model for $f$  and then define an algorithm that tries to minimize the loss function \n",
    "\n",
    "From a very high level we can think of the \"model\" as \n",
    "\n",
    "$$f(\\bx;\\bb) = \\sum_{m=1}^p \\beta_m 1[\\bx \\in R_m]$$\n",
    "\n",
    "in which we are interested to learn both $p$ and the regions $R_i$. The resultant optimization problem is **non-convex** and rather than\n",
    "parameterising the regions, one typically uses an algorithm that then dictates the regions. \n",
    "\n",
    "A common approach (CART) is to use **forward search**, a heuristic optimization approach that is also used for variable selection in regression:\n",
    "+ To grow the tree one level more, all possible variables are considered, for each the optimal cutoff point is found and then the variable that achieves largest decrease in loss function is chosen to determine the next level\n",
    "+ This is heuristic but very fast to implement\n",
    "\n",
    "This approach is by no means immune to overfitting - and we have seen this already.\n",
    "+ One approach is to bound the depth or the total number of leaves\n",
    "+ Another is to use a **backward search** after the forward, effectively a pruning step that tries to remove branches. The backward criterion is effectively a penalized likelihood criterion based again on a **regularization parameter**. \n",
    "+ As usual, a predictive measure is used to estimate the regularizing parameter (depth/backward parameter), which again relies on some estimate of MSE - e.g. CV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Behind the scenes\n",
    "\n",
    "+ Non-convex optimisation and heuristics\n",
    "  + forward-backward search\n",
    "  +  model fit criteria for tree-based models (e.g. regression and classification)\n",
    "  +  model complexity criteria for tree-based models; regularisation parameter\n",
    "+ Variable selection and importance\n",
    "+ Categorical predictors and missing data\n",
    "\n",
    "Lets revisit the sklearn class DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Complexity, accuracy and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here we show *overfitting* risk: allowing an increasing number of final leaves in our tree, we increase the accuracy on training data, but not necessarily increase accuracy when applying to new (test) data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# some sklearn tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# keep the results in a list\n",
    "complexity_value = []\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "\n",
    "# loop through possible values of max_leaf_nodes\n",
    "for max_leaf_nodes in range (2, 200): \n",
    "    model  = DecisionTreeClassifier(criterion = \"entropy\", max_leaf_nodes=max_leaf_nodes) \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    #predict both on train and test set\n",
    "    y_pred = model.predict(Xtest)\n",
    "    y_pred_train = model.predict(X)\n",
    "    \n",
    "    # store the data to be used for plotting\n",
    "    train_accuracy.append(accuracy_score(y, y_pred_train))\n",
    "    test_accuracy.append(accuracy_score(ytest, y_pred))\n",
    "    complexity_value.append(max_leaf_nodes)\n",
    "    \n",
    "\n",
    "plt.plot(complexity_value, train_accuracy, label='Train accuracy')\n",
    "plt.plot(complexity_value, test_accuracy, label='Test accuracy')\n",
    "plt.xlabel(\"complexity\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Implicit variable selection and scoring\n",
    "\n",
    "The model and algorithm effectively do a variable selection, since some variables drop out (never chosen for split) and a scoring of those that enter the model. Before we go into the mechanics of these procedures, lets see some results for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model  = DecisionTreeClassifier(max_leaf_nodes=20) \n",
    "model.fit(X, y)\n",
    "important_features = pd.DataFrame(model.feature_importances_/model.feature_importances_.max() ,index=X.columns, columns=['importance'])\n",
    "# it is common to normalize by the importance of the highest\n",
    "important_features.sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Lets see the tree too\n",
    "\n",
    "Image(plot_tree(model, feature_names, class_labels))\n",
    "# do right click save image as and open outside the notebook to see details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Stability \n",
    "\n",
    "Lets see how stable the previous results are to different subsets of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_accuracy_argmax = [] # the maximal test accuracy achieved for each split\n",
    "importance_char = [] # the variable char_! importance\n",
    "\n",
    "for bootsam in np.arange(100):\n",
    "    # split randomly dataset; do not fix the seed to see variation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    #\n",
    "    # First search for depth \n",
    "    test_accuracy = []\n",
    "    complexity_value = []\n",
    "    for max_leaf_nodes in np.arange (5, 40): \n",
    "        model  = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes) \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        complexity_value.append(max_leaf_nodes)\n",
    "    test_accuracy_argmax.append(complexity_value[np.argmax(test_accuracy)])\n",
    "    #\n",
    "    #print(f\"Optimum max leaf {complexity_value[np.argmax(test_accuracy)]}\")\n",
    "    # Then find and store the relative importance of fare for the chosen tree\n",
    "    \n",
    "    model  = DecisionTreeClassifier(max_leaf_nodes=complexity_value[np.argmax(test_accuracy)]) \n",
    "    model.fit(X_train, y_train)\n",
    "    important_features = pd.DataFrame(model.feature_importances_/model.feature_importances_.max() ,index=X.columns, columns=['importance'])\n",
    "    importance_char.append(important_features.loc[\"char_freq_!\",:].values[0])\n",
    "    \n",
    "# Print the results in a convenient manner\n",
    "result =pd.DataFrame(test_accuracy_argmax,columns=[\"depth\"])\n",
    "result[\"score_char\"] = importance_char\n",
    "result.plot(x=\"depth\",y=\"score_char\",kind=\"scatter\")\n",
    "\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## From tree to forest: Bagging and random forests\n",
    "\n",
    "The earlier experiment that \"growed\" a tree-based model on different subsets of the data revealed an important aspect of tree-based models, that of *instability*, small changes in the data (e.g. removing 20% of observations) causes large changes in the learned model.\n",
    "\n",
    "Here is another example with some simulated data from Hastie, Tibshirani and Friedman book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"bagged_trees.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is typical of an estimator with high variance\n",
    "\n",
    "+ The bias-variance tradeoff in ML \n",
    "\n",
    "  + The population-averaged tree-based model \n",
    "  \n",
    "B(ootstrap) AGG(regation) ING tries to estimate the population-averaged estimator by boostrapping the procedure and averaging across datasets. The resultant learned function is not anymore a decision tree, but a linear combination of trees. This makes bagging an ensemble method - effectively a *model averaging* approach. \n",
    "\n",
    "\n",
    "Bagging does not change the bias of the underlying estimator, but it can decrease its variance, hence improve prediction performance. \n",
    "\n",
    "This is what happens on the same simulated dataset of Hastie, Tibshirani and Friedman\n",
    "\n",
    "<img src=\"bagged_trees_error.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The tree-based estimators from each bootstrap sample are correlated with each other. If they are significantly positively correlated we should not expect any advantages from bagging. Hence, we should try to produce tree-based estimators with as little correlation between them as possible. This is what *random forests* try and do, by forcing the growth of trees to evolve to different \"species\" across the different bootstrap samples. \n",
    "\n",
    "Estimators that exhibit significant instability can benefit from bagging and random forests since it is more likely that the results on different bootstrap samples will be considerably different from eachother. \n",
    "\n",
    "Algorithmically, *random forest* require a small change to the common forward-backward heuristic optimisation algorithm used for growining trees. At forward steps instead of considering all variables as candidates for splitting, choose a subset each time and consider only those.The resultant trees, one for each bootstrap sample, are averaged to produce the random forest estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Random forests in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(31415) # impose random seed for reproducibility\n",
    "\n",
    "dataset = pd.read_csv(\"spam_small_train.csv\")\n",
    "X = dataset.drop(['class'], axis = 1)\n",
    "y = dataset[\"class\"]\n",
    "\n",
    "# to print stats\n",
    "feature_names = X.columns\n",
    "class_labels = [\"email\", \"spam\"]\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=20)\n",
    "scores = cross_val_score(forest, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "importances = forest.fit(X,y).feature_importances_\n",
    "important_features = pd.Series(data=importances/importances.max() ,index=feature_names)\n",
    "important_features.sort_values(ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Excercise 1\n",
    "As an exercise, check how accuracy is affected when dropping the features that are less important (i.e. importance less than 0.05). Increase the reported decimals if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Boosting \n",
    "\n",
    "This is another way to reduce the variance of an estimator and increase predictive accuracy. This has links to a number of key ideas in machine learning:\n",
    "\n",
    "+ class imbalance in classification by \"boosting\", rather than the class, the data that is hard to classify\n",
    "+ bagging, by doing various passes through the data and averaging the estimators\n",
    "+ forward search for heuristic optimisation, as it turns out that a popular implementation does exactly that\n",
    "+ linear models with regularisation, as it turns out it is a linear combination of features, partially linear in the parameters, with a regulariser for complexity \n",
    "\n",
    "It can be applied to many ML methods, but it works really well with trees and the combination of the two yields often the best performing approach to predictive modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Boosting and additive models\n",
    "\n",
    "The essense of the method is as follows. Say we have models $f_m(x,\\beta_m)$, for example each could be a tree-based model. We wish to fit a linear combination of those to data, i.e., \n",
    "\n",
    "$$\n",
    "f(x,\\theta,\\beta) = \\sum_{m=1}^p \\theta_m f_m(x,\\beta_m)\n",
    "$$\n",
    "\n",
    "where $f_m$ stands for the *m_th* model and $\\theta_m$ is the corresponding weight.\n",
    "\n",
    "Following the fundamental principles of machine learning modelling we also need a data-function mismatch component in the model, for example: \n",
    "\n",
    "+ in regression $L(y_i,f(x_i)) = (y_i-f(x_i))^2$ is the deviance\n",
    "+ in classification, with output coded as $y_i \\in \\{+1,-1\\}$, $L(y_i,f(x_i)) = \\log(1+e^{-y_i f(x_i,\\beta)})$ is also the negative log-likelihood.\n",
    "\n",
    "Therefore, to learn such additive model from data we need to solve \n",
    "\n",
    "$$\n",
    "\\arg \\min_{\\theta,\\beta} \\sum_{i=1}^n L\\left( y_i, \\sum_{m=1}^p \\theta_m f_m(x,\\beta_m) \\right)\n",
    "$$\n",
    "\n",
    "In other words, we need to find threshold cuts, and weight of each classifier, so that overall we minimize the loss function.\n",
    "\n",
    "This is a hard, non-convex optimization. We can see boosting and its various versions as iterative optimization algorithms for fitting such additive model to data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Possibilities\n",
    "\n",
    "+ Forward search for heuristic optimization: fit the model one term at a time. Combined with the exponential loss, $L(y_i,f(x_i)) = e^{-y_i f(x_i)}$, this yields the so-called *AdaBoost* (Adaptive Boosting). Main highlights of AdaBoost are that:\n",
    "\n",
    "    + Each tree is a simple 'decision stump', so only one node per tree (really weak classifier);\n",
    "    + Each iteration we re-weight observations, up-weighting the ones with lower accuracy in previous trees.\n",
    "\n",
    "+ Gradient-based minimization: this yields *gradient boosting*.  Gradient boosting optimizes weights of observations in next tree by using gradients in the loss function, applied to all trees that have been trained up to that point. GBM divides the optimization problem into two parts by first determining the direction of the step and then optimizing the step length. \n",
    "\n",
    "+ Extreme Gradient Boosting (*XGBoost*): Similar to GBM but each iteration solves the gradient search in one single step using Taylor expansion of the gradient formula. On the other hand, it uses a more regularized model formalization to control over-fitting (l1, l2 parameters), which typically implies better performance.\n",
    "\n",
    "The risk of overfitting on those models has to be treated by using some regularization parameters (e.g. to limit the number of leaves) \n",
    "\n",
    "In particular, there is an issue called over-specialization, which means that trees added at later iterations tend to impact the prediction of only a few observations, so that their contribution towards the rest of the dataset becomes negligible. This is only addressed by Extreme Gradient Boosting model, by using regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Lets see boosting in action for the spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "np.random.seed(3123) # impose random seed for reproducibility\n",
    "\n",
    "# Uncomment to test the effect of removing less important variables\n",
    "# X = dataset.drop(['class','word_freq_hpl','word_freq_re','word_freq_edu','word_freq_you','word_freq_our'], axis = 1)\n",
    "# feature_names = X.columns\n",
    "\n",
    "tree = GradientBoostingClassifier(n_estimators=50)\n",
    "scores = cross_val_score(tree, X, y, cv=10,  scoring='roc_auc')\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "importance = tree.fit(X,y).feature_importances_\n",
    "important_features = pd.Series(data=importance/importance.max() ,index=feature_names)\n",
    "important_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise 2\n",
    "As an exercise, check how accuracy is affected when dropping the features that are less important (i.e. importance less than 0.05). Increase the reported decimals if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Some hints for practitioners\n",
    "\n",
    "### Models\n",
    "#### CART (single tree)\n",
    "Use *sklearn* function $DecisionTreeClassifier$ for classification, or $DecisionTreeRegressor$ for regression. These models are simpler to understand and explore but typically yield worse results than the ones based on 'multiple tree' approach. Some interesting parameters  to tune (for classification) are:\n",
    "+ *max_depth*: Number of levels in a tree\n",
    "+ *min_samples_split*: Minimum number of samples left to try a new split\n",
    "+ *min_samples_leaf*: Minimum number of samples allowed in a leaf\n",
    "+ *min_impurity_decrease*: Don't accept the split if we don't reach a minimum decrease in impurity\n",
    "+ *min_impurity_split*: Early stop criteria, don't split if mininum impurity has been reached\n",
    "+ *class_weight*: Use 'balanced' if classes are unbalanced.\n",
    "\n",
    "\n",
    "#### Random forest\n",
    "Use *sklearn* function $RandomForestClassifier$ for classification, or $RandomForestRegressor$ for regression. Main parameters are:\n",
    "+ *n_estimators*: Number of trees to build\n",
    "+ *max_features*: Number of features to consider when looking for the best split. If less than 100%, this intruduces stochasticity.\n",
    "+ *boostrap*: Keep default 'True' to allow boostrapping the sample for each tree, useful to prevent overfitting\n",
    "+ *oob_score*: If True, yields a proxy for expected accuracy, using *out of bag* observations per tree.\n",
    "+ *n_jobs*: Number of CPU cores to be used (parallelization).\n",
    "+ *max_depth*, *min_samples_split*, *min_impurity_decrease*, *min_impurity_split*, *class_weight*: Similar to CART\n",
    "\n",
    "\n",
    "#### AdaBoost\n",
    "Use *sklearn* function $AdaBoostClassifier$ for classification, or $AdaBoostRegressor$ for regression. Main parameters are:\n",
    "+ *n_estimators*: Number of trees to build\n",
    "+ *learning_rate*: Intensity of the re-weighting (boosting) each time we build a new tree.\n",
    "+ 'base_estimator': Weak learner, by default 'DecisionTreeClassifier(max_depth=1)', but one can for instance increase depth to check if this helps.\n",
    "\n",
    "#### GradientBoosting (GBM)\n",
    "Use *sklearn* function $GradientBoostingClassifier$ for classification, or $GradientBoostingRegressor$ for regression. Main parameters are:\n",
    "+ *n_estimators*: Number of trees to build\n",
    "+ *subsample*: Percentage of samples to use for every tree.\n",
    "+ *learning_rate*: similar to AdaBoost\n",
    "+ *max_features*,*max_depth*, *min_samples_split*, *min_impurity_decrease*, *min_impurity_split*, *class_weight*: Similar to random forest\n",
    "\n",
    "#### Extreme Gradient Boosting (XGBoost)\n",
    "Use *xgboost* package and follow documentation at https://xgboost.readthedocs.io/en/latest/python/\n",
    "\n",
    "Main functions are $xgboost.XGBClassifier$ or $xgboost.XGBRegressor$, which are an API to be used with *sklearn*.\n",
    "\n",
    "Main parameters are:\n",
    "+ *gamma*: similar to min_impurity_decrease. Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "+ *colsample_bytree*:  Percentage of features to consider when constructing each tree.\n",
    "+ *reg_alpha*:  L1 regularization term on weights to control overfitting\n",
    "+ *reg_lambda*: L2 regularization term on weights to control overtiffing\n",
    "+ *n_estimators*,*subsample*, *learning_rate*,*max_depth*: Similar to Gradient Boosting. \n",
    "\n",
    "#### Other MART (Multiple Additive Regression Trees) models:\n",
    "+ *ligthgbm* package: Computationally faster than GBM, https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "+ *catboost* package: Computationally faster than GBM, allows using GPU, https://github.com/catboost/catboost and https://catboost.ai/\n",
    "+ Extremely Randomized Trees: Similar to random forest but optimization of splits is based on discrete random thresholds instead of exploring the entire space, so it's faster (and sometimes more accurate since is less prone to overfit). Use *sklearn* function $ExtraTreesClassifier$ or *ExtraTreesRegressor*\n",
    "\n",
    "### Hyperparameter optimization\n",
    "Use *sklearn* functions such as $GridSearchCV$ or $RandomizedSearchCV$. See example above in random forest section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Chapters 4.5, 9.1, 9.2, 10 and 15.  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Chen, T., Guestrin, C., 2016. *XGBoost: A Scalable Tree Boosting System*. Proceedings at Conference of Knowledge Discovery and Data Mining, August 13-17, 2016, San Francisco, CA, USA. https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf\n",
    "\n",
    "XGBoost tutorials. https://xgboost.readthedocs.io/en/latest/tutorials/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "DecisionTrees.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "548.867px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
