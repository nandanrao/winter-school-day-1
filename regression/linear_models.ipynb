{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"bgsedsc_0.jpg\">\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\bu}{\\boldsymbol{u}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-dimensional predictive regression models\n",
    "\n",
    "We start implementing the first models/algorithms for prediction, and understand to some depth their methodological underpinning \n",
    "\n",
    "\n",
    "For the implementations we will rely on another Python toolkit\n",
    "\n",
    "\n",
    "http://scikit-learn.org/\n",
    "\n",
    "A python library that provides a variety of tools  for machine learning (e.g., pre-process data, evaluate models, etc), and implements a lot of ML algorithms \n",
    "\n",
    "More information can be found here\n",
    " http://scikit-learn.org/stable/documentation.html\n",
    " \n",
    " *Warning: pandas are great for data management and sklearn is useful for ML but they are not (yet) perfectly compatible. sklearn works with numpy objects*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module we build linear models for regression as linear combination of features extracted from original input data. Key ideas we develop here is how to learn the model and how to evaluate model performance. We discuss the bias-variance tradeoff, and related notions such as stability, overfitting, regularization. We introduce a nice algorithmic framework for predictive modelling with a large number of features, that of penalized likelihood generally and the lasso specifically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the relevant modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "#seaborn is a module for figures\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the curve data\n",
    "\n",
    "These are our *training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = pd.read_csv('curve_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot here is a DF method\n",
    "cdata.plot(x='x',y='y',kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first learning function: linear in $x$\n",
    "\n",
    "$$ y_i \\sim \\Gau(f(x_i,\\bb) , v)$$\n",
    "\n",
    "$$ f(x,\\mathbf{\\beta}) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "$$ \\bb = (\\beta_0,\\beta_1)^T$$\n",
    "\n",
    "Remark on the notation: bold-face for vectors, otherwise scalars; bold-face capital letters for matrices\n",
    "\n",
    "We first import the relevant tools. For predictive modelling, which is the aim here, `LinearRegression` is good enough. I would not use this for inference though. `statsmodels.api` appears a better choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the relevant sklearn tools\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the class structure\n",
    "regr  = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the data for sklearn - you can appreciate the earlier \n",
    "#\"not very compatible\" comment\n",
    "# we create an array with data provided by the DF\n",
    "data = np.array(cdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create now predictors and response - and make sure they are \n",
    "# in the right format - they are not by default hence the reshape\n",
    "X = data[:,0].reshape(10,1)\n",
    "y = data[:,1].reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not have to define a column of 1s since intercept can be added\n",
    "# in the options\n",
    "# notice the application of the method to the regr instance\n",
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting new data\n",
    "\n",
    "We will compute the *learning function* $f(x,\\bb)$ on some *test data* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose some points to predict - notice reshape and -1! \n",
    "X_new = 0.01*np.arange(100).reshape(-1, 1)\n",
    "#the learned function f(x) at prediction inputs\n",
    "f_new_pred = regr.predict(X_new) \n",
    "# Plot the random data\n",
    "plt.figure()\n",
    "# plot training data\n",
    "plt.scatter(X, y, c=\"orange\", label=\"training data\", alpha=0.5)\n",
    "# plot predictions \n",
    "plt.plot(X_new, f_new_pred, c=\"red\", label=\"test data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better learning function: linear in parameters and features, non-linear in input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y_i \\sim \\Gau(f(x_i,\\bb) , v)\n",
    "\\end{equation}\n",
    "\n",
    "$$ f(x,\\bb) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots \\beta_n x^{p-1} $$\n",
    "\n",
    "The following is a constructive perspective: we create **features**, that is new input variables that are transformations of the original ones. In the above construction the vector of features for the $i$th data point are \n",
    "\n",
    "$$\\bphi_i =(1,x_i,x_i^2,\\ldots,x_i^{p-1})^T$$ \n",
    "\n",
    "then \n",
    "\n",
    "$$ f(x_i,\\bb) = \\bb^T \\bphi_i$$\n",
    "\n",
    "Notice that we now have $p$ predictors, even though $x$ is 1-dimensional. The choice of polynomial features is simply for illustration; in fact this is not such a good choice for a number of reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting more features for the given input. \n",
    "# Polynomial features are so common that sklearn has a built in function\n",
    "# for constructing them \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures as plf\n",
    "# the argument specifies the polynomial order, here we choose up to power 3\n",
    "poly = plf(3)\n",
    "F = poly.fit_transform(X) #F for fearure matrix\n",
    "print(F) # notice that the intercept is now added by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr  = LinearRegression(fit_intercept=False)\n",
    "regr.fit(F,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with the new model: note that we need to generate features\n",
    "# for the test data\n",
    "X_new = 0.01*np.arange(100).reshape(-1, 1)\n",
    "F_new = plf(3).fit_transform(X_new)\n",
    "f_new_pred = regr.predict(F_new) \n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"orange\", label=\"training data\", alpha=0.5)\n",
    "plt.plot(X_new, f_new_pred, c=\"red\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behind the scenes: the learning function, convexity and algorithm\n",
    "\n",
    "In more generality: output $y_i$ (1-d), input $\\bx_i$ (multi-d), features vector for each individual $i$, $\\bphi_i$, a linear model for the learning function $f(\\bx_i,\\bb) = \\bphi_i^T \\bb$\n",
    "\n",
    "The first part of the model $y_i \\sim \\Gau(f(\\bx_i,\\bb) , v)$ quantifies the datapoint-model match. Every data point **scores** the model by how predictable it is by the model, i.e., according to the density $p(y_i | \\bx_i)$; the higher the density the better the model predicts the data point. \n",
    "\n",
    "It is more convenient to work in a different scale: \n",
    "\n",
    "$$-\\log p(y_i \\mid \\bx_i)$$\n",
    "\n",
    "Now this a **datapoint-model mismatch**, the lower this is the better. \n",
    "The overall **data-model mismatch** is obtained by aggregating the evidence by all data. Assuming that the data are independent we have that \n",
    "\n",
    "$$p(y_1,\\ldots,y_n \\mid \\bx_1,\\ldots,\\bx_n) = p(y_1 \\mid \\bx_1) \\cdots p(y_n\\mid \\bx_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having also assumed Gaussian errors, \n",
    "\n",
    "$$-\\log p(y_i | \\bx_i) = {1 \\over 2 v}(y_i - f(\\bx_i,\\bb))^2 - {1 \\over 2} \\log v - {1\\over 2} \\log(2 \\pi)$$\n",
    "\n",
    "from which we obtain the **loss function**, here  the **negative log-likelihood**\n",
    "\n",
    "$$L(\\bb,v) =  {1 \\over 2 v} \\sum_{i=1}^n (y_i - f(\\bx_i,\\bb))^2 - {n \\over 2} \\log v$$\n",
    "\n",
    "This is a nice function to optimize. First, note that one can optimize over $\\bb$ regardless of the value of $v$: \n",
    "\n",
    "\n",
    "$$\n",
    "\\whbb = \\arg \\min_\\bb L(\\bb,v)\n",
    "$$\n",
    "\n",
    "Once $\\whbb$ has been obtained, we can easily optimize over $v$.  $L(\\bb,v)$ as a function of $\\bb$ is **convex**, in fact it is a quadratic form and one way to optimize is to reduce the computation to a solution of a linear system  \n",
    "\n",
    "What `LinearRegression.fit` does is solve the least-squares minimization problem. \n",
    "Then, the learning function is estimated by \n",
    "\n",
    "$$\\hf_n(\\bx) = f(\\bx,\\whbb)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Lets plot the training $y_i$ vs $\\hf_n(\\bx_i)$. We will also report the **square of the correlation coefficient** between the two samples. This squared correlation coefficient is used so frequently that has a name: coefficient R-squared\n",
    "\n",
    "A small calculation shows that \n",
    "\n",
    "$$R-squared = 1 - {\\sum_i (y_i - \\hf_n(\\bx_i))^2 \\over \\sum_i (y_i - \\ybar)^2}$$\n",
    "\n",
    "hence large $R-squared$ is equivalent to small **sum of squared errors**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted vs observed - y_hat = f(x)\n",
    "\n",
    "y_hat = regr.predict(F)\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat[:,0]))\n",
    "plt.title('R-squared equals %.3f' %rho**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big question is - statistically speaking - what do these sample statistics, such as R-squared, estimate and whether they are doing a good job at estimating it \n",
    "\n",
    "The following are some *population* quantities worth estimating. Let $(x^*,y^*)$ be a randomly chosen *test* datapoint from the same phenomenon that has generated the *training data* $(x_i,y_i),i=1,\\ldots,n$. What would be interesting to compute - if possible - is \n",
    "\n",
    "+ Mean Squared Error (MSE):\n",
    "\n",
    "$$ MSE_n = \\E[(y^* - \\hf_n(\\bx^*))^2]$$ \n",
    "\n",
    "+ Squared correlation between $y^*$ and $\\hf_n(\\bx^*)$:\n",
    "\n",
    "$$ R^2 = \\Cor(y^*,\\hf_n(\\bx^*))^2$$\n",
    "\n",
    "Replacing population expectation with sample averages, and using the training data as samples, we get the quantities we introduced earlier. Therefore, those are statistical estimators of the population quantities above. Are they any good though? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will return to this question but for the time being lets consider an alternative estimator of these quantities, the so-called **leave-one-out cross-validation estimator**. Intuitively it is simple: \n",
    "\n",
    "+ Use all data points but the $i$th to estimate the learning function: \n",
    "\n",
    "$$\n",
    "\\hf_{n-1,-i}(\\bx)\n",
    "$$\n",
    "\n",
    "+ Using the estimated learning function, predict the $i$th training data point: \n",
    "\n",
    "$$\\hf_{n-1,-i}(\\bx_i)$$\n",
    "\n",
    "+ Estimate the MSE or $R^2$ by computing\n",
    "\n",
    "$$(y_i - \\hf_{n-1,-i}(\\bx_i))^2$$\n",
    "\n",
    "+ We can actually do this for each data point $i$ and then average the estimates: \n",
    "\n",
    "$${1 \\over n} \\sum_{i=1}^n (y_i - \\hf_{n-1,-i}(\\bx_i))^2$$ \n",
    "\n",
    "We can implement this ourselves - or use some `sklearn` functions too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "y_hat_cv = cvp(regr, F, y, cv=10) # this is leave-one-out CV when cv=10      \n",
    "                                  # and 10 because n=10\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat_cv) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat_cv[:,0]))\n",
    "plt.title('Leave-one-out CV R-squared equals %.3f' %rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a more flexible model\n",
    "\n",
    "We repeat this analysis but working with polynomial of order 7; in this case the number of parameters is almost the same as number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 7\n",
    "poly = plf(order)\n",
    "Flarge = poly.fit_transform(X)\n",
    "regr.fit(Flarge,y)\n",
    "\n",
    "X_new_= 0.01*np.arange(100).reshape(-1, 1)\n",
    "F_new = plf(order).fit_transform(X_new)\n",
    "f_new_pred = regr.predict(F_new) \n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"orange\", label=\"training data\", alpha=0.5)\n",
    "plt.plot(X_new, f_new_pred, c=\"red\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually the estimated learning function looks good and very similar to the one we obtained with the 3rd order polynomial. \n",
    "\n",
    "Visual inspection is **crucial** - but often not an option. Lets look at our model evaluation criteria too - these are pretty much always available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = regr.predict(Flarge)\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat[:,0]))\n",
    "plt.title('R-squared equals %.3f' % rho**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_cv = cvp(regr, Flarge, y, cv=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat_cv) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat_cv[:,0]))\n",
    "plt.title('Leave-one-out CV R-squared equals %.3f' % rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting!! This is example of what we might call loosely **overfitting** but more things are going on here (e.g. leverage). What should note is that at this *model complexity* the procedure we have followed to estimate the learning function has become very **unstable**: it is overly sensitive to a small change in the training data, and in particular to two data points \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Pick one of the two extremal points in the input space, the ones close to a y value of 0, learn the model and plot the solution for the leave-one-out case. See how the estimated function varies when each of the points are left out and compare with the estimated function when all the data are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index to the point to be excluded from model\n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Remove this position from input data\n",
    "\n",
    "\n",
    "# Retrain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bias-variance tradeoff in Statistics and Machine Learning\n",
    "\n",
    "The picture tells it all! (Taken from Bishop's book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bias_variance_bishop.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is typical: \n",
    "\n",
    "+ *Procedures* with *few degrees of freedom* are stable but the learning function they estimate can be systematically far off from the optimal one (**bias**). They would have comparable R-squared and leave-one-out CV R-squared\n",
    "\n",
    "+ *Procedures* with *high degrees of freedom* are sensitive to training data (**variance**) but the learning function they estimate might not have systematic differences from the optimal one. They would have near-1 R-squared and near-0 leave-one-out CV R-squared \n",
    "\n",
    "It is important to understand that these properties involve **both the model and the loss function** - this is why I tactically used the vague term *procedure* above: it is the combination of both - what we might call \"algorithm\" - that matters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure, taken also from Bishop, shows the estimated mean squared error - blue is in-sample, red is analogous to leave-one-out CV - for increasing values of $p$ (denoted by $M$ in the fig). \n",
    "\n",
    "<img src=\"bishop_overfit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want **algorithms that can strike a good bias-variance tradeoff**!\n",
    "\n",
    "The remaining of this lecture is devoted to:\n",
    "\n",
    "1. Giving such algorithms: e.g. the so-called LASSO; they use the same linear-in-features model but a different loss function\n",
    "2. Discussing how to estimate $MSE_n$ from data; we will revisit CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A framework for good predictive algorithms: penalized likelihood methods\n",
    "\n",
    "We now see *algorithms* that achieve a good bias-variance tradeoff and allow us to fit linear models with very large number of features, even much larger than the number of observations - e.g $p \\approx e^n$. The key structure that they try to exploit, and do well when this structure is consistent with the data, is that of **sparsity**, i.e., that only a small number of terms in the linear model are needed to get good predictions. The training will select the few important features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These *algorithms* use the *same linear models* we have seen before. But they use *different loss functions* \n",
    "\n",
    "We focus on the coefficients $\\bb$: since their estimation did not require knowledge of $v$, we simplify (and rescale) the loss function we derived earlier to \n",
    "\n",
    "$$L(\\bb) =  {1 \\over 2 n} \\sum_{i=1}^n (y_i - f(\\bx_i,\\bb))^2$$ \n",
    "\n",
    "The class of algorithms we discuss now are in the family of so-called **shrinkage methods**; they are based on changing the loss function to \n",
    "\n",
    "$$L(\\bb) =  {1 \\over 2 n} \\sum_{i=1}^n (y_i - f(\\bx_i,\\bb))^2 + \\lambda \\sum_{j=0}^{p-1} g(\\beta_j)$$ \n",
    "\n",
    "where $g(\\beta_j)$ is a **penalty** term, that penalizes $\\beta_j$ when $\\beta_j \\neq  0$; recall that $\\beta_j = 0$ means that feature $j$ (e.g., $j-1$ polynomial order) is dropped from the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This family of loss functions are called penalized likelihood. The following are some common examples of penalties - and the names the corresponding algorithms are known with: \n",
    "\n",
    "+ LASSO: $g(\\beta) = |\\beta|$\n",
    "\n",
    "+ ridge regression: $g(\\beta) = \\beta^2$\n",
    "\n",
    "+ Elastic Net/SCAD/MC+/Reciprocal LASSO/...\n",
    "\n",
    "Again the picture says it all: \n",
    "\n",
    "<img src = \"penalties.png\" width =\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some remarks: \n",
    "\n",
    "+ Feature standardization: \n",
    "    + Different coefficients are penalized in the same way: this only makes sense if the different coefficients are comparable. Consider the following silly example that makes the point: suppose I want to build a simple predictive model for the time it takes to get with my bike from my home to a given location in Barcelona just in terms of the vertical dispacement, $v$, and horizontal dispacement $h$, in terms of data $(y_i,v_i,h_i)$ of times it took in past trips $y_i$ when the displacements were $v_i$ and $h_i$. My house is on the beach and I mostly move along the coast, so I decided to record the horizontal discplacement in kilometers and the vertical dispacement in meters. My model is \n",
    "    $$ y_i = \\beta_1 h_i + \\beta_2 v_i + error$$\n",
    "        I should expect that $\\beta_1 \\approx 1000 \\beta_2$ - they will be on completely different scales.\n",
    "        \n",
    "    + Penalized likelihood algorithms require that the features have been standardized to have comparable scales. We often subtract the sample mean and divide by the standard deviation across replications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The role of $\\lambda$:\n",
    "    + This **hyperparameter** allows us to trade bias with variance, creating a continuum of mean squared errors along which we try to choose an optimal $\\lambda$ - hence an optimal predictive model. \n",
    "    + $\\lambda \\to 0$ leads to small bias/large variance, $\\lambda \\to \\infty$ to large bias/small variance. \n",
    "    + Lets revisit now the bias-variance tradeoff picture: ridge regression with varying $\\lambda$s: \n",
    "    <img src=\"bias_variance_bishop.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso in action: the curve data with many many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 9\n",
    "poly = plf(order)\n",
    "Flarge = poly.fit_transform(X)[:,1:] # drop the intercept column\n",
    "\n",
    "# standardisation of input is critical: We will use sklearn to do this\n",
    "\n",
    "# generic lasso regression object\n",
    "from sklearn.preprocessing import scale as scl\n",
    "Flarge = scl(Flarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "#alpha is what was lambda in our notation\n",
    "regr_lasso = Lasso(alpha=0.0001, fit_intercept=False,warm_start=True,max_iter=1000000)\n",
    "\n",
    "# application to our data and model\n",
    "regr_lasso.fit(Flarge,y)\n",
    "\n",
    "# see coefficients\n",
    "print(regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV R2 \n",
    "\n",
    "y_hat_cv = cvp(regr_lasso, F, y, cv=10)\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat_cv) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat_cv))\n",
    "plt.title('R-squared equals %.3f' %rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further insights & observations on lasso\n",
    "\n",
    "+ Sparsity: increasing values of $\\lambda$ have the effect that an increasing number of estimated coefficients are exactly zero\n",
    "+ Variable selection: hence, implictly lasso also performs a principled feature selection - but this is not an aspect we will explore here\n",
    "    + Lets see these properties in action in our example. Lets look at the coefficients for a range of $\\lambda$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path \n",
    "\n",
    "alphas, _, coefs = lars_path(Flarge, y[:,0], method='lasso', \n",
    "                             verbose=True, max_iter = 100000)\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.xlim(0,0.001)\n",
    "plt.ylim(-3,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convexity: the loss function is convex; this is because the least squares function is convex (a quadratic function) and the penalty is convex too. This allows very efficient estimation using **convex optimization** algorithms. \n",
    "    + A common choice is **coordinate-wise descent**. This is an iterative algorithm that scans through each coefficient and updates it using information about the values of all other coefficients. \n",
    "    + For standardized features $\\bx_1,\\bx_2,\\ldots$ each coefficient is updated as: \n",
    "$$\\beta_j \\leftarrow \\mathcal{S}_{\\lambda}\\left({1 \\over n} \\boldsymbol{r}_{-j}^T \\bx_j\\right )$$\n",
    "      where $\\boldsymbol{r}_{-j}$ is the vector of residuals from the model with $\\beta_j = 0$ and the soft-thresholding operator is:\n",
    "      $$\\mathcal{S}_{\\lambda}(\\beta) = \\mathrm{sign}(\\beta) \\max\\{|\\beta| - \\lambda,0\\}$$\n",
    "    + The fast optimization is a major attraction for the lasso \n",
    "      + Coordinate-wise descent is implemented at a cost that grows only linearly in $n$ and $p$: it is a practical solution for Big Data and Big Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the regularization hyperparameter\n",
    "\n",
    "Given an estimator of $MSE_n$ we can choose $\\lambda$ to achieve an MSE as small as possible. For example leave-one-out cross validation. \n",
    "\n",
    "Another possibility is to use a **model selection** criterion. Model selection criteria balance in-sample fit with **model complexity**. \n",
    "\n",
    "First, we try leave-one-out CV in our example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out CV selection of $\\lambda$ for the curve data\n",
    "\n",
    "Lets try and do this using the leave-one-out CV we have already discussed. We will try a range of different $\\lambda$s, for each of which we will estimate the MSE by leave-one-out CV, plot the resultant curve and try to identify a good $\\lambda$\n",
    "\n",
    "The procedure is computationally intensive - this will not manifest here where $n=10$\n",
    "\n",
    "We will use `GridSearchCV` to carry out the outer (grid search and CV) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# you may wonder why I set random_state now and did not do so before\n",
    "lasso = Lasso(random_state=0,max_iter=3000000) \n",
    "alphas = np.array([0.000007, 0.00002, 0.00004, 0.00005,0.00008,0.0001,0.00012, 0.00015,0.0002,0.00025,0.0003,0.0004,0.0005,0.0006,0.0007,0.002])\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "n_folds = 10 # remember that for this dataset this is leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scorer to evaluate performance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer \n",
    "\n",
    "## ALWAYS read carefully documentation. copying here from make_scorer\n",
    "## greater_is_better : boolean, default=True\n",
    "# \"Whether score_func is a score function (default), meaning high is \n",
    "# good, or a loss function, meaning low is good. \n",
    "# In the latter case, the scorer object will sign-flip \n",
    "# the outcome of the score_func.\n",
    "mse = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, scoring = mse, \n",
    "                   cv=n_folds, refit=False)\n",
    "\n",
    "clf.fit(Flarge, y)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "std_error = scores_std / np.sqrt(n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines([0.00003] ,ymin, ymax, linestyle='dashed')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Refit the lasso with the regularised *alpha* ($\\lambda$) parameter that we just found that maximizes the CV score. Check the number of parameters that have been shrunk to zero.\n",
    "\n",
    "Report the leave-one-out CV $R^2$ coefficient. Plot leave-one-out  predicted versus actual output values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refitting the lasso with that regularising parameter\n",
    "# Hint: use Lasso()\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV R2 and plot \n",
    "# Hint: Use cvp() on the previous model, then plot y versus y_hat_cv as we did before\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some hints for practitioners\n",
    "\n",
    "\n",
    "+ Building good predictive models with hundreds or even thousands of features is a real possibility\n",
    "+ LASSO combines least squares fit with a penalty for model complexity; it relies on an additional *regularizing hyperparameter*\n",
    "+ Sklearn module `LinearRegression` can be used for predictive modelling. `Lasso` can be used to fit a lasso model for given value of regularization hyperparameter. `lars_path` can return all the possible lasso solutions for all values of the regularization hyperparameter and is a useful tool in exploring the different models\n",
    "+ The choice of regularization hyperparameter is a model choice problem; you can use both cross validation to estimate the MSE for each possible value of the hyperparameter and use a grid search to identify good values for the hyperparameter - `GridSearchCV` is useful wrapper for this. Less data and computationally intensive method is to use a model selection criterion, e.g. AIC, and a simple formula exists for the lasso\n",
    "+ For inference with a linear model, i.e., obtaining confidence intervals, p-values etc, `LinearRegression` is  entirely inappropriate. Use other modules, e.g., `statsmodels.api`. \n",
    "+ Inference with the output of the lasso model is non-trivial and subject of more advanced material. Although lasso implictly selects a model by dropping variables, you should not over-interpret the variables that have been selected. Its merit is primary in getting a good predictive model. Lasso is helpful in screening some variables, so it is often used as a first step to be followed by a more formal selection procedure. Generally, these questions fall under the theme of *post-selection* inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming project: real estate assesment evaluation\n",
    "\n",
    "Home valuation is key in real estate industry, and also the basis for mortgages in credit sector. Here we have to predict the estimated value of a property.\n",
    "\n",
    "\n",
    "Data (*Regression_Supervised_Train.csv*) consist of a list of features plus the resulting $parcelvalue$, described in *Case_data_dictionary.xlsx* file. Each row corresponds to a particular home valuation, and $transactiondate$ is the date when the property was effectively sold. Properties are defined by $lotid$, but be aware that one property can be sold more than once (it's not the usual case). Also notice that some features are sometime empty, your model has to deal with it.\n",
    "\n",
    "Note that you don't have to use $totaltaxvalue$, $buildvalue$ or $landvalue$, because they are closely correlated with the final value to predict.\n",
    "\n",
    "+ Using this data build a predictive model for $parcelvalue$ \n",
    "+ In your analysis for faster algorithms use the AIC criterion for choosing any hyperparameters \n",
    "+ Try a first quick implementation, then try to optimize hyperparameters\n",
    "+ For this analysis there is an extra test dataset. Once your code is submitted we will run a competition to see how you score in the test data. Hence have prepared also the necessary script to compute the MSE estimate on the test data once released.\n",
    "+ Bonus: Try an approach to fill NA without removing features or observations, and check improvements.\n",
    "\n",
    "You can follow those **steps** in your first implementation:\n",
    "1. *Explore* and understand the dataset. Report missing data\n",
    "2. As a simplified initial version, get rid of *missing data* by:\n",
    "    + Removing features that have more than 40% of missing data\n",
    "    + After that, removing observations that have missing data\n",
    "3. Create *dummy variables* for relevant categorical features\n",
    "4. *Build* your model and test it on the same input data\n",
    "5. Assess expected accuracy using *cross-validation*\n",
    "6. Report which variable impacts more on results \n",
    "7. Prepare the code to *run* on a new input file and be able to report accuracy, following same preparation steps (missing data, dummies, etc)\n",
    "\n",
    "You may want to iterate to refine some of these steps once you get performance results in step 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here (click on the window and type 'b' if you want to split in more than one code window)\n",
    "\n",
    "# Step 1: Read data, report missing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove features with missing data, and then observations with missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create dummies for relevant features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build your model and get predictions from train data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Assess expected accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Report variable impact\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare code to run and check performance of you model using a new input data with same exact format\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Chapters 1, 2, Section 3.4; More advanced 3.8,3.9,7.10  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Bishop, C.M. *Pattern recognition and machine learning*. Chapter 1, Sections 3.1, 3.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
